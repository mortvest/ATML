\documentclass[a4paper]{article}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{amsthm}
\usepackage{amssymb}
\usepackage[english]{babel}
\usepackage{float}
\usepackage{graphicx}
\usepackage{grffile}
\usepackage{hyperref}
\usepackage[utf8]{inputenc}
\usepackage{listings}
\usepackage{xcolor}
%% \usepackage{subfigure}
\usepackage{pdfpages}
\usepackage{graphicx}
\usepackage{subcaption}
\usepackage{stmaryrd}
\usepackage{a4wide}

\lstset{
  frame=tb,
  language=Python,
  aboveskip=3mm,
  belowskip=3mm,
  showstringspaces=false,
  formfeed=newpage,
  tabsize=4,
  comment=[l]{\#},
  breaklines=true,
  basicstyle=\small
}
\renewcommand{\thesubsubsection}{\alph{subsubsection}}
\newcommand{\prob}[1]{\mathbb{P}\left(#1\right)}
\newcommand{\expect}[1]{\mathbb{E}\left(#1\right)}
\newcommand{\avg}[1]{\sum_{i=1}^{#1}X_i}
\newcommand{\dotpr}[2]{\langle #1,\; #2 \rangle}
\newcommand{\norm}[1]{\left\lVert#1\right\rVert}
\newcommand*{\QEDA}{\hfill\ensuremath{\blacksquare}}%
\newcommand*{\bs}[1]{\boldsymbol{#1}}


\title{\vspace{-5cm}ATML Home Assignment 6}
\author{Dmitry Serykh (qwl888)}

\begin{document}
\maketitle
\section{CMA-ES for Reinforcement Learning}
\label{sec:1}

\section{Tighter analysis of the Hedge algorithm}
\label{sec:2}
\subsection{}
\label{subsec:21}
Let us recall Lemma 2.6 from the lecture notes:\\\\
Let $X$ be a random variable, such that $X \in [a,b]$, then for any $\lambda \in \mathbb{R}$:
\begin{align}
  \label{hoeff}
  \mathbb{E}\left[e^{\lambda X}\right] \leq e^{\lambda \mathbb{E}[X]+\frac{\lambda^{2}(b-a)^{2}}{8}}
\end{align}
We follow the proof of Lemma 5.2 from the lecture notes, until we reach (5.2):
\begin{align*}
  \frac{W_t}{W_{t-1}} &= \sum_a e^{-\eta X_t^a}p_t(a)\\
  &= \mathbb{E}_{p_t}[e^{-\eta X_t^a}]
\end{align*}
Then we can apply the Hoeffding's lemma, where the values of $X_t^{a} \in [0,1]$,
and $\lambda = -\eta$. Hence we get:
\begin{align*}
  \mathbb{E}_{p_t}[e^{-\eta X_t^a}] &\leq e^{-\eta \expect{X_t^a} + \frac{\eta^2}{8}}\\
  &=e^{-\eta \sum_a X_t^a p_t(a) + \frac{\eta^2}{8}}
\end{align*}
Then:
\begin{align*}
  \frac{W_{T}}{W_{0}}=\frac{W_{1}}{W_{0}} \times \frac{W_{2}}{W_{1}} \times
  \cdots \times \frac{W_{T}}{W_{T-1}} &\leq
  e^{-\eta \sum_{t=1}^{T} \sum_{a} X_{t}^{a} p_{t}(a)+ \sum_{t=1}^{T}\frac{\eta^{2}}{8}}
\end{align*}
Then, we combine this result with the other inequality from the proof in the lecture
notes and apply the logarithm:
\[
-\eta \min _{a} L_{T}(a)-\ln K \leq
-\eta \sum_{t=1}^{T} \sum_{a} X_{t}^{a} p_{t}(a)+
\frac{\eta^{2}}{8} T
\]
Then, we divide by eta and change sides:
\[
\sum_{t=1}^{T} \sum_{a} X_{t}^{a} p_{t}(a)-\min _{a} L_{T}(a)
\leq
\frac{\ln K}{\eta}+\frac{\eta}{8}T
\]
And we get:
\[
\mathbb{E}\left[R_{T}\right] \leq \frac{\ln K}{\eta}+\frac{\eta}{8} T
\]

\subsection{}
\label{subsec:22}
Then we find the value of $\eta$ that minimizes the bound by first finding the
first and second derivatives of the right side:
\begin{align*}
  \frac{d}{d\eta} \left[\frac{\ln{K}}{\eta} + \frac{\eta}{8}T\right]&=
  \frac{T}{8} - \frac{\ln{K}}{\eta^2}\\
  \frac{d}{d\eta} \left[\frac{T}{8} - \frac{\ln{K}}{\eta^2} \right]
  &= \frac{2\ln{K}}{\eta^3} \geq 0 \tag{since $\eta > 0$ and $K>1$}
  %% \frac{2\ln{K)}{\eta^3}
\end{align*}
We set the first derivative equal to zero and solve for $\eta$:
\begin{align*}
\frac{T}{8} - \frac{\ln{K}}{\eta^2} &= 0\\
\ln{K} &= \frac{T}{8}\eta^2\\
\eta &= \sqrt{\frac{8\ln{K}}{T}}
\end{align*}
Since second derivative is positive, the extremal point is a minimum.
\subsection{}
\label{subsec:23}
By setting $\eta$ to the value that we found previously in the bound, we get:
\begin{align*}
  \mathbb{E}\left[R_{T}\right] &\leq
  \frac{\ln K}{\sqrt{\frac{8\ln{K}}{T}}}+\frac{\sqrt{\frac{8\ln{K}}{T}}}{8} T\\
  &= \frac{8\ln{K} + 8\ln{K}} {8\sqrt{\frac{8\ln{K}}{T}}}\\
  &= \frac{2\ln{K}}{\sqrt{\frac{8\ln{K}}{T}}}\\
  &= \sqrt{T} \frac{\ln{K}}{\sqrt{2\ln{K}}}\\
  &= \sqrt{\frac{1}{2}T} \frac{\ln{K}}{\sqrt{\ln{K}}}\\
  &= \sqrt{\frac{1}{2}T \ln{K}}
\end{align*}
Which is exactly the bound that we should obtain.

\section{The doubling trick}
\label{sec:3}
\subsection{}
\label{subsec:31}
We will first prove a lemma that would be useful later. In the prove we will use
the fact that we are allowed to square both sides of inequality if they are positive.
Let $m\geq0$, then:
\begin{align*}
  \sqrt{2}^m -1 \leq \sqrt{2^m -1}\\
  \sqrt{2^m} -1 \leq \sqrt{2^m -1}\\
  (\sqrt{2}^m -1)^2 \leq 2^m -1\\
  2^m - 2\sqrt{2^m} + 1 \leq 2^m - 1\\
  -2\sqrt{2^m} \leq  -2\\
  \sqrt{2^m} \geq  1\\
  2^m \geq  1
\end{align*}
This is clearly true, since $m\geq 0$.\\\\
With $\eta_{m}=\sqrt{\frac{8 \ln N}{2^{m}}}$, the expected regret of Hedge
within the period $(2^m,...,2^{m+1}-1)$ is bounded by $\sqrt{\frac{1}{2} 2^m\ln{K}}$.
Then for any $T=2^m - 1$, within the period $(1,...,T)$, we have:
\begin{align*}
  \expect{R_T} &\leq \sum_{j=0}^{j=m-1} \sqrt{\frac{1}{2}2^m\ln{K}}\\
  &=\sqrt{\frac{1}{2}\ln{K}}\sum_{j=0}^{j=m-1} \sqrt{2}^m\\
  &=\sqrt{\frac{1}{2}\ln{K}} \cdot \frac{1-\sqrt{2}^m}{1-\sqrt{2}} \tag{sum o
    geom. series}\\
  &=\sqrt{\frac{1}{2}\ln{K}} \left( \frac{1}{1-\sqrt{2}} - \frac{\sqrt{2}^m}{1-\sqrt{2}} \right)\\
  &=\sqrt{\frac{1}{2}\ln{K}} \left(
  \frac{1}{\sqrt{2}-1} \cdot  (\sqrt{2}^m -1)
  \right)\\
  &\leq \sqrt{\frac{1}{2}\ln{K}} \cdot \frac{1}{\sqrt{2} - 1} \sqrt{2^m - 1}
  \tag{by lemma, since $m\geq0$}\\
  &=\frac{1}{\sqrt{2} - 1} \sqrt{\frac{1}{2}T\ln{K}}
\end{align*}
\QEDA

\subsection{}
\label{subsec:32}
Any time period can be described as $T = 2^m + T_C$, for $m\geq0$ and
$0 \leq T_C < 2^m$. Let $T' = 2^{m+1}-1$, then $T \leq T'$ and
$\expect{R_T} \leq \expect{R_{T'}}$.
Then, we can use the results from previous subsection and get:
\begin{align*}
  \mathbb{E}\left[R_{T}\right]&\leq \mathbb{E}\left[R_{T'}\right]\\
  &\leq\frac{1}{\sqrt{2}-1} \sqrt{\frac{1}{2} T' \ln K}\\
  &= \frac{1}{\sqrt{2}-1} \sqrt{\frac{1}{2} (2^{m+1}-1) \ln K}\\
  &\leq\frac{1}{\sqrt{2}-1} \sqrt{\frac{1}{2} (2^{m+1}+2T_C) \ln K} \tag{since $T_C \geq0$}\\
  &\leq\frac{\sqrt{2}}{\sqrt{2}-1} \sqrt{\frac{1}{2} (2^m+T_C) \ln K}\\
  &\leq\frac{\sqrt{2}}{\sqrt{2}-1} \sqrt{\frac{1}{2} T \ln K}
\end{align*}
\QEDA
\section{Empirical evaluation of algorithms for adversarial environments}
\label{sec:4}

\section{Empirical comparison of UCB1 and EXP3 algorithms}
\label{sec:5}
\begin{figure}
  \centering
  \begin{subfigure}[b]{0.49\textwidth}
    \centering
    \includegraphics[width=\textwidth]{code/plt_k2_mu025}
  \end{subfigure}
  \begin{subfigure}[b]{0.49\textwidth}
    \centering
    \includegraphics[width=\textwidth]{code/plt_k2_mu0375}
  \end{subfigure}
  \begin{subfigure}[b]{0.49\textwidth}
    \centering
    \includegraphics[width=\textwidth]{code/plt_k2_mu04375}
  \end{subfigure}
  \begin{subfigure}[b]{0.49\textwidth}
    \centering
    \includegraphics[width=\textwidth]{code/plt_k4_mu025}
  \end{subfigure}
  \begin{subfigure}[b]{0.49\textwidth}
    \centering
    \includegraphics[width=\textwidth]{code/plt_k4_mu0375}
  \end{subfigure}
  \begin{subfigure}[b]{0.49\textwidth}
    \centering
    \includegraphics[width=\textwidth]{code/plt_k4_mu04375}
  \end{subfigure}
  \caption{Plots for Exercise 5}
  \label{plt5}
\end{figure}


\end{document}

%% \section{Empirical comparison of UCB1 and EXP3 algorithms}
%% \label{sec:5}
%% \begin{figure}
%%   \centering
%%   \begin{subfigure}[b]{0.49\textwidth}
%%     \centering
%%     \includegraphics[width=\textwidth]{code/plt_k2_mu025}
%%   \end{subfigure}
%%   \begin{subfigure}[b]{0.49\textwidth}
%%     \centering
%%     \includegraphics[width=\textwidth]{code/plt_k2_mu0375}
%%   \end{subfigure}
%%   \caption{Plots for Exercise 5}
%%   \label{plt5}
%% \end{figure}

%% \begin{lstlisting}[caption="Calculation of g"]
%% def calc_g(Xs, y, w):
%%     N = np.shape(Xs)[0]
%%     # use matrix X of xs instead of for-loop = much faster
%%     X = np.c_[Xs, np.ones(N)]
%%     num = y.T * X
%%     denum = 1 + np.exp(y * (w @ X.T))
%%     M = num.T/denum
%%     # return mean of each row
%%     return (-1 * np.mean(M, axis=1))
%% \end{lstlisting}
