\documentclass[a4paper]{article}
\usepackage{algorithmic}
\usepackage[]{algorithm2e}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{amsthm}
\usepackage{amssymb}
%% \usepackage{bbold}
\usepackage[english]{babel}
\usepackage{float}
\usepackage{graphicx}
\usepackage{grffile}
\usepackage{hyperref}
\usepackage[utf8]{inputenc}
\usepackage{listings}
\usepackage{xcolor}
\usepackage{pdfpages}
\usepackage{graphicx}
\usepackage{subcaption}
\usepackage{stmaryrd}
\usepackage{a4wide}

%% \lstset{
%%   frame=tb,
%%   language=Python,
%%   aboveskip=3mm,
%%   belowskip=3mm,
%%   showstringspaces=false,
%%   formfeed=newpage,
%%   tabsize=4,
%%   comment=[l]{\#},
%%   breaklines=true,
%%   basicstyle=\small
%% }
\renewcommand{\thesubsubsection}{\alph{subsubsection})}
\newcommand{\prob}[1]{\mathbb{P}\left(#1\right)}
\newcommand{\expect}[1]{\mathbb{E}\left(#1\right)}
\newcommand{\avg}[1]{\sum_{i=1}^{#1}X_i}
\newcommand{\dotpr}[2]{\langle #1,\; #2 \rangle}
\newcommand{\norm}[1]{\left\lVert#1\right\rVert}
\newcommand*{\QEDA}{\hfill\ensuremath{\blacksquare}}%
\newcommand*{\bs}[1]{\boldsymbol{#1}}

%% algorithms
\RestyleAlgo{boxruled}
\SetKwInput{KwInput}{Input}          
\SetKwInput{KwOutput}{Output} 
\SetKwInput{KwInit}{Initialization}           


\title{\vspace{-5cm}ATML Home Assignment 7}
\author{Dmitry Serykh (qwl888)}

\begin{document}
\maketitle
\section{Policy Gradient Methods}
\label{sec:1}
\subsection*{Question 1.1}
\label{subsec:11}
Function $\pi(s,a)$ is defined as:
\[
\pi(s, a)=\frac{e^{\boldsymbol{\theta}^{\mathrm{T}} \boldsymbol{\phi}(s, a)}}
   {\sum_{b} e^{\boldsymbol{\theta}^{\mathrm{T}} \boldsymbol{\phi}(s, b)}}
\]
then:
\[
\pi(s, b)=\frac{e^{\boldsymbol{\theta}^{\mathrm{T}} \boldsymbol{\phi}(s, b)}}
   {\sum_{b} e^{\boldsymbol{\theta}^{\mathrm{T}} \boldsymbol{\phi}(s, b)}}
\]
I start by finding the derivative using a CAS:
\begin{align*}
  \frac{\partial \pi(s, a)}{\partial \bs{\theta}} &=
  \frac{\bs{\phi}(s,a) e^{\boldsymbol{\theta}^{\mathrm{T}} \boldsymbol{\phi}(s, a)}}
       {\sum_{b} e^{\boldsymbol{\theta}^{\mathrm{T}} \boldsymbol{\phi}(s, b)}}
  -
  \frac{e^{\boldsymbol{\theta}^{\mathrm{T}} \boldsymbol{\phi}(s, a)}
        \sum_{b} \boldsymbol{\phi}(s, b)e^{\boldsymbol{\theta}^{\mathrm{T}} \boldsymbol{\phi}(s, b)}}
       {(\sum_{b} e^{\boldsymbol{\theta}^{\mathrm{T}} \boldsymbol{\phi}(s, b)})^2}\\
       &=\frac{e^{\boldsymbol{\theta}^{\mathrm{T}} \boldsymbol{\phi}(s, a)}}
              {\sum_{b} e^{\boldsymbol{\theta}^{\mathrm{T}} \boldsymbol{\phi}(s, b)}}
         \left(
         \boldsymbol{\phi}(s, a)-
         \frac{\sum_{b} \boldsymbol{\phi}(s, b)e^{\boldsymbol{\theta}^{\mathrm{T}} \boldsymbol{\phi}(s, b)}}
              {\sum_{b} e^{\boldsymbol{\theta}^{\mathrm{T}} \boldsymbol{\phi}(s, b)}}
              \right)\\
  &=
   \pi(s, a)\left(
  \boldsymbol{\phi}(s, a)-
  \frac{\sum_{b} \boldsymbol{\phi}(s, b)e^{\boldsymbol{\theta}^{\mathrm{T}} \boldsymbol{\phi}(s, b)}}
       {\sum_{b} e^{\boldsymbol{\theta}^{\mathrm{T}} \boldsymbol{\phi}(s, b)}}
       \right)\\
  &=
   \pi(s, a)\left(
  \boldsymbol{\phi}(s, a)-
  \sum_{b} \frac{\boldsymbol{\phi}(s, b)e^{\boldsymbol{\theta}^{\mathrm{T}} \boldsymbol{\phi}(s, b)}}
       {\sum_{b} e^{\boldsymbol{\theta}^{\mathrm{T}} \boldsymbol{\phi}(s, b)}}
       \right)\\
  &=
  \pi(s, a)\left(
  \boldsymbol{\phi}(s, a)-
  \sum_{b} \frac{e^{\boldsymbol{\theta}^{\mathrm{T}} \boldsymbol{\phi}(s, b)}}
      {\sum_{b} e^{\boldsymbol{\theta}^{\mathrm{T}} \boldsymbol{\phi}(s, b)}
      }\boldsymbol{\phi}(s, b)
  \right)\\
  &=
  \pi(s, a)\left(
  \boldsymbol{\phi}(s, a)-
  \sum_{b}
  \pi(s, b)
  \boldsymbol{\phi}(s, b)
  \right)
\end{align*}
I can then use the compatibility condition to find the partial derivative:
\begin{align*}
  \frac{\partial f_{\boldsymbol{w}}(s, a)}{\partial \boldsymbol{w}}
    &=\frac{\partial \pi(s, a)}{\partial \boldsymbol{\theta}} \frac{1}{\pi(s, a)}\\
    &= \pi(s, a)\left(\boldsymbol{\phi}(s, a)-\sum_{b}\pi(s, b)\boldsymbol{\phi}(s, b)\right)
  \frac{1}{\pi(s, a)}\\
  &=\boldsymbol{\phi}(s, a)-\sum_{b}\pi(s, b)\boldsymbol{\phi}(s, b)
\end{align*}
\QEDA

\subsection*{Question 1.2}
\label{subsec:12}
I assume that:
\[
f_{w}(s, a)=\boldsymbol{w}^{\mathrm{T}}\left[\bs{\phi}(s, a)-\sum_{b} \pi(s, b) \bs{\phi}(s, b)\right]
\]
then:
\begin{align*}
  \sum_{a} \pi(s, a) f_{\boldsymbol{w}}(s, a)
  &=
  \sum_{a} \pi(s, a) \boldsymbol{w}^{\mathrm{T}}
  \left[\bs{\phi}(s, a)-\sum_{b} \pi(s, b) \bs{\phi}(s, b)\right]\\
  &=
  \sum_{a} \pi(s, a) \boldsymbol{w}^{\mathrm{T}}
  \bs{\phi}(s, a)-
  \sum_{a} \pi(s, a) \boldsymbol{w}^{\mathrm{T}}
  \sum_{b} \pi(s, b) \bs{\phi}(s, b)
\end{align*}
Hence, my objective can be re-formulated into proving following for all $s\in S$:
\begin{align*}
  \sum_{a} \pi(s, a) \boldsymbol{w}^{\mathrm{T}}\bs{\phi}(s, a)
  &=
  \sum_{a} \pi(s, a) \boldsymbol{w}^{\mathrm{T}}\sum_{b} \pi(s, b) \bs{\phi}(s, b)\\
  \boldsymbol{w}^{\mathrm{T}} \sum_{a} \pi(s, a) \bs{\phi}(s, a)
  &=
   \boldsymbol{w}^{\mathrm{T}}\sum_{a} \pi(s, a)\sum_{b} \pi(s, b) \bs{\phi}(s, b)\\
   \sum_{a} \pi(s, a) \bs{\phi}(s, a)
  &=
   \sum_{a} \pi(s, a)\sum_{b} \pi(s, b) \bs{\phi}(s, b)\\
   \sum_{b} \pi(s, b) \bs{\phi}(s, b)
  &=
   \sum_{a} \pi(s, a)\sum_{b} \pi(s, b) \bs{\phi}(s, b)\\
   \sum_{a} \pi(s, a)
  &=
   \frac{\sum_{b} \pi(s, b) \bs{\phi}(s, b)}{\sum_{b} \pi(s, b) \bs{\phi}(s, b)}\\
   \sum_{a}\pi(s, a) &= 1
\end{align*}
Which is clearly true for all $s \in S$, because 
   $\sum_{a}\pi(s, a)$ is a sum of probabilities of taking events in state $s$,
which must sum up to one.
\QEDA
\subsection*{Question 1.3}
\label{subsec:13}
If $f_{\boldsymbol{w}}$ is linear in $\frac{\partial}{\partial
  \boldsymbol{\theta}} \ln \pi(s, a)$, then: 
\[
f_{\boldsymbol{w}} = \bs{w}^T \left(\frac{\partial}{\partial \boldsymbol{\theta}} \ln \pi(s, a)\right)
\]
I can then find the derivative:
\begin{align*}
  \frac{\partial f_{\bs{w}} (s,a)}{\partial \boldsymbol{w}} &=
  \frac{\partial}{\partial \boldsymbol{w}} \bs{w}^T
  \left(\frac{\partial}{\partial \boldsymbol{\theta}} \ln \pi(s, a)\right)\\
  &=
  \frac{\partial}{\partial \boldsymbol{\theta}} \ln \pi(s, a)\\
  &= \frac{\partial \pi(s,a)}{\partial
    \bs{\theta}}\frac{1}{\pi(s, a)} \tag{using chain rule}
\end{align*}
Which is the requirement of the \emph{compatibility condition}.\QEDA

\section{Offline Evaluation of Bandit Algorithms}
\label{sec:2}
\subsection{}
\label{subsec:21}
I came up with following reasons for why this could be undesirable:
\begin{itemize}
\item It could be risky to execute an algorithm on unknown quality in a live environment.
  This could potentially lead to monetary or other losses if the new
  algorithm ends up being bad in practice. That could be partially avoided by
  using offline evaluation.
\item The amount of feedback that could be generated in a live environment could
  be limited. If we use importance or rejection sampling, we can potentially run it on
  huge amounts of data that was gathered earlier.
\item In order to compare the quality of two algorithms, we want the input data
  of both algorithms to be coming from the same distribution. That can not be
  guaranteed in a live environment. Furthermore, a potential adversary could
  influence our algorithm choice by playing specifically to new algorithm's
  strengths/weaknesses during the live evaluation.
  We could consequently over-/underestimate the actual performance of the new
  algorithm and risk choosing a suboptimal one.
\end{itemize}

\subsection{}
\label{subsec:22}
\subsubsection*{a)}
I start by transforming the EXP3 and USB1 algorithms to work with importance
sampling.
The pseudocode for the modified algorithms can be seen of Algorithm \ref{algo1}
and \ref{algo2} respectively.
\begin{figure}[ht]
\begin{algorithm}[H]
 \caption{Modified \textbf{EXP3}}
 \label{algo1}
 \DontPrintSemicolon
 \KwInput{Learning rates $\eta_1 \geq \eta_2 \geq \dots >0$}
 $\forall a: L_0(a) = 0$\;
 \For{$t = 1,2,\dots$ }{
   $\forall a: p_t(a)=
   \frac{e^{-\eta_{t} L_{t-1}(a)}}{\sum_{a^{\prime}} e^{-\eta_{t} L_{t-1}\left(a^{\prime}\right)}}$\;
   Sample $A_t$ according to $p_t$ and play it\;
   Observe and suffer $\ell_t^{A_t}$\;
   Set $\tilde{\ell}_t^a = \frac{\ell_t^a \mathbb{I}(A_t=a)}{p_t(a)}
   \begin{cases} \frac{\ell_t^a}{p_t(a)} & \text{if } A_t=a \\
                      0                 & \text{otherwise}
   \end{cases}$\;
   $\forall a: \tilde{L}_t(a) = \tilde{L}_{t-1}(a) + \tilde{\ell}^a_t$
 }
\end{algorithm}
\end{figure}

\begin{figure}[ht]
\begin{algorithm}[H]
 \caption{Modified \textbf{UCB1}}
 \label{algo2}
 \DontPrintSemicolon
 \KwInit{Play each action once}
 \For{$t = K + 1, K + 2,\dots$ }{
   $\text {Set } A_{t}=\arg \max _{a} \hat{\mu}_{t-1}(a)+\sqrt{\frac{3 \ln t}{2 N_{t-1}(a)}}$\;
   Observe and suffer $\ell_t^{A_t}$\;
 }
\end{algorithm}
\end{figure}
Then, I will find the regret bound for the modified EXP3.\\\\
I get the inequality:
\[
\expect{R_T} \leq \frac{\ln{K}}{\eta} + \frac{\eta}{2}K^2T
\]
I wish to find the value of $\eta$ that minimizes the RHS of the inequality.
I first find the derivatives:
\begin{align*}
  \frac{d}{d\eta} &= \frac{K^2T}{2} - \frac{\ln{K}}{\eta^2}\\
  \frac{d^2}{d\eta} &= \frac{2\ln{K}}{\eta^3}
\end{align*}
Then, solve the inequality:
\begin{align*}
  \frac{K^2T}{2} - \frac{\ln{K}}{\eta^2} &= 0\\
  \frac{K^2T}{2} &= \frac{\ln{K}}{\eta^2}\\
  \eta^2 &= \frac{2\ln{K}}{K^2T}\\
  \eta &= \sqrt{\frac{2\ln{K}}{K^2T}}
\end{align*}
The second derivative is positive since $K, \eta > 0$, hence the extremal point
is a minimum. I substitute the found value of $\eta$ in order to obtain the
bound:
\begin{align*}
  \expect{R_T} &\leq 
  \frac{K}{\sqrt{\frac{2\ln{K}}{K^2T}}} +
  \frac{\sqrt{\frac{2\ln{K}}{K^2T}}}{2} K^2T \\
  &= \frac{2\ln{K}}{\sqrt{\frac{2\ln{K}}{K^2T}}}\\
  &= \sqrt{2}\frac{\ln{K}}{\sqrt{\frac{\ln{K}}{K^2T}}}\\
  &= \sqrt{2 K^2 T \ln{K}}
\end{align*}
I then modify the algorithm into the anytime EXP3, hence the learning rate
becomes:
\[
  \eta_t = \sqrt{\frac{\ln{K}}{K^2t}}
\]
while the regret bound becomes:
\[
  \expect{R_t} \leq 2\sqrt{K^2 t \ln{K}}
\]

\subsubsection*{b-d)}

\subsubsection*{e)}

\end{document}

%% \section{Empirical comparison of UCB1 and EXP3 algorithms}
%% \label{sec:5}
%% \begin{figure}
%%   \centering
%%   \begin{subfigure}[b]{0.49\textwidth}
%%     \centering
%%     \includegraphics[width=\textwidth]{code/plt_k2_mu025}
%%   \end{subfigure}
%%   \begin{subfigure}[b]{0.49\textwidth}
%%     \centering
%%     \includegraphics[width=\textwidth]{code/plt_k2_mu0375}
%%   \end{subfigure}
%%   \caption{Plots for Exercise 5}
%%   \label{plt5}
%% \end{figure}

%% \begin{lstlisting}[caption="Calculation of g"]
%% def calc_g(Xs, y, w):
%%     N = np.shape(Xs)[0]
%%     # use matrix X of xs instead of for-loop = much faster
%%     X = np.c_[Xs, np.ones(N)]
%%     num = y.T * X
%%     denum = 1 + np.exp(y * (w @ X.T))
%%     M = num.T/denum
%%     # return mean of each row
%%     return (-1 * np.mean(M, axis=1))
%% \end{lstlisting}
