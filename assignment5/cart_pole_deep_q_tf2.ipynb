{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cart Pole with neural function approximator TF2\n",
    "### Christian Igel, 2019\n",
    "\n",
    "If you have suggestions for improvement, [let me know](mailto:igel@diku.dk).\n",
    "\n",
    "I took inspiration from https://github.com/udacity/deep-learning/blob/master/reinforcement/Q-learning-cart.ipynb."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/local/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/gym/envs/registration.py:14: PkgResourcesDeprecationWarning: Parameters to load are deprecated.  Call .resolve and .require separately.\n",
      "  result = entry_point.load(False)\n"
     ]
    }
   ],
   "source": [
    "# Create the Cart-Pole game environment\n",
    "env = gym.make('CartPole-v0')\n",
    "action_size = 2\n",
    "state_size = 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's just test the environment first:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "initial state: [ 0.02762808 -0.02371627 -0.01010676  0.03572369]\n",
      "return:  14.0\n",
      "initial state: [-0.01412401 -0.00438035  0.00329317  0.02733238]\n",
      "return:  11.0\n",
      "initial state: [-0.00178016  0.04981552  0.01295902 -0.0419306 ]\n",
      "return:  12.0\n",
      "initial state: [-0.00813216 -0.01081521 -0.03332785  0.01818648]\n",
      "return:  14.0\n",
      "initial state: [ 0.0416307   0.01266761 -0.03418146  0.03306596]\n",
      "return:  11.0\n",
      "initial state: [-0.02205869  0.0405169  -0.04732866 -0.00874136]\n",
      "return:  21.0\n",
      "initial state: [ 0.03805057 -0.02327617  0.03319597  0.02106177]\n",
      "return:  11.0\n",
      "initial state: [0.01707813 0.00938839 0.0257548  0.00098258]\n",
      "return:  22.0\n",
      "initial state: [-0.03655643 -0.01076983  0.0145503   0.02198126]\n",
      "return:  15.0\n",
      "initial state: [-0.00513509 -0.03844208 -0.04870138 -0.04599407]\n",
      "return:  36.0\n"
     ]
    }
   ],
   "source": [
    "test_episodes = 10\n",
    "for _ in range(test_episodes):\n",
    "    R = 0\n",
    "    state = env.reset()  # Environment starts in a random state, cart and pole are moving\n",
    "    print(\"initial state:\", state)\n",
    "    while True:  # Environment sets \"done\" to true after 200 steps \n",
    "        # Uncomment the line below to watch the simulation\n",
    "        # env.render()\n",
    "        state, reward, done, info = env.step(env.action_space.sample()) # take a random action\n",
    "        R += reward\n",
    "        if done:\n",
    "            print(\"return: \", R)\n",
    "            env.reset()\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.close()  # Closes the visualization window"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define *Q* network architecture:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QNetwork(tf.keras.Model):\n",
    "    def __init__(self, state_size=4, action_size=2, hidden_size=10, name='QNetwork'):\n",
    "        super(QNetwork, self).__init__()\n",
    "        self.fc1 = tf.keras.layers.Dense(hidden_size, activation='relu', use_bias=True)\n",
    "        self.fc2 = tf.keras.layers.Dense(hidden_size, activation='relu', use_bias=True)\n",
    "        self.fc3 = tf.keras.layers.Dense(action_size, activation=None, use_bias=True)\n",
    "    def call(self, x):\n",
    "        return self.fc3(self.fc2(self.fc1(x)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data structure for storing experiences:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import deque\n",
    "class Memory():\n",
    "    def __init__(self, max_size = 1000):\n",
    "        self.buffer = deque(maxlen=max_size)\n",
    "    \n",
    "    def add(self, experience):\n",
    "        self.buffer.append(experience)\n",
    "            \n",
    "    def sample(self, batch_size):\n",
    "        idx = np.random.choice(np.arange(len(self.buffer)), \n",
    "                               size=batch_size, \n",
    "                               replace=False)\n",
    "        return [self.buffer[ii] for ii in idx]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define basic constants:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_episodes = 400           # max number of episodes to learn from\n",
    "max_steps = 200                # max steps in an episode\n",
    "gamma = 0.99                   # future reward discount\n",
    "\n",
    "# Exploration parameters\n",
    "explore_start = 1.0            # exploration probability at start\n",
    "explore_stop = 0.005           # minimum exploration probability \n",
    "decay_rate = 0.001             # exponential decay rate for exploration prob\n",
    "\n",
    "# Network parameters\n",
    "hidden_size = 64               # number of units in each Q-network hidden layer\n",
    "\n",
    "# Memory parameters\n",
    "memory_size = 10000            # memory capacity\n",
    "batch_size = 100               # experience mini-batch size\n",
    "pretrain_length = batch_size   # number experiences to pretrain the memory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instantiate network:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"q_network\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense (Dense)                multiple                  320       \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              multiple                  4160      \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              multiple                  130       \n",
      "=================================================================\n",
      "Total params: 4,610\n",
      "Trainable params: 4,610\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "mainQN = QNetwork(name='main', hidden_size=hidden_size)\n",
    "mainQN.build(input_shape=(None, state_size))\n",
    "print(mainQN.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Do the *Q*-learning:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the simulation\n",
    "env.reset()\n",
    "# Take one random step to get the pole and cart moving\n",
    "state, reward, done, _ = env.step(env.action_space.sample())\n",
    "\n",
    "memory = Memory(max_size=memory_size)\n",
    "\n",
    "# Make a bunch of random actions and store the experiences\n",
    "for _ in range(pretrain_length):\n",
    "    # Uncomment the line below to watch the simulation\n",
    "    # env.render()\n",
    "\n",
    "    # Make a random action\n",
    "    action = env.action_space.sample()\n",
    "    next_state, reward, done, _ = env.step(action)\n",
    "\n",
    "    if done:\n",
    "        # The simulation fails, so no next state\n",
    "        next_state = np.zeros(state.shape)\n",
    "        # Add experience to memory\n",
    "        memory.add((state, action, reward, next_state))\n",
    "        \n",
    "        # Start new episode\n",
    "        env.reset()\n",
    "        # Take one random step to get the pole and cart moving\n",
    "        state, reward, done, _ = env.step(env.action_space.sample())\n",
    "    else:\n",
    "        # Add experience to memory\n",
    "        memory.add((state, action, reward, next_state))\n",
    "        state = next_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.0001         # Q-network learning rate\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate, epsilon=0.01)\n",
    "\n",
    "@tf.function\n",
    "def compute_gradient(states, actions, targets, action_size=2):\n",
    "    one_hot_actions = tf.one_hot(actions, action_size)\n",
    "    with tf.GradientTape() as tape:\n",
    "        output = mainQN(states)\n",
    "        Q = tf.reduce_sum(tf.multiply(output, one_hot_actions), axis=1)\n",
    "        loss = tf.math.reduce_mean(tf.square(Q - targets))\n",
    "    return loss, tape.gradient(loss, mainQN.trainable_variables)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now train with experiences:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 0 Total reward: 17.0 Training loss: 1.0609 Explore P: 0.9832\n",
      "Episode: 1 Total reward: 22.0 Training loss: 1.0621 Explore P: 0.9619\n",
      "Episode: 2 Total reward: 12.0 Training loss: 1.0890 Explore P: 0.9505\n",
      "Episode: 3 Total reward: 28.0 Training loss: 1.0588 Explore P: 0.9244\n",
      "Episode: 4 Total reward: 20.0 Training loss: 1.0548 Explore P: 0.9062\n",
      "Episode: 5 Total reward: 46.0 Training loss: 1.0314 Explore P: 0.8657\n",
      "Episode: 6 Total reward: 29.0 Training loss: 1.0561 Explore P: 0.8411\n",
      "Episode: 7 Total reward: 74.0 Training loss: 1.0863 Explore P: 0.7815\n",
      "Episode: 8 Total reward: 71.0 Training loss: 1.0655 Explore P: 0.7282\n",
      "Episode: 9 Total reward: 14.0 Training loss: 1.0651 Explore P: 0.7182\n",
      "Episode: 10 Total reward: 26.0 Training loss: 1.0266 Explore P: 0.6999\n",
      "Episode: 11 Total reward: 36.0 Training loss: 1.0417 Explore P: 0.6753\n",
      "Episode: 12 Total reward: 24.0 Training loss: 1.1641 Explore P: 0.6594\n",
      "Episode: 13 Total reward: 27.0 Training loss: 1.0755 Explore P: 0.6420\n",
      "Episode: 14 Total reward: 40.0 Training loss: 1.1141 Explore P: 0.6170\n",
      "Episode: 15 Total reward: 22.0 Training loss: 1.1863 Explore P: 0.6037\n",
      "Episode: 16 Total reward: 22.0 Training loss: 1.1533 Explore P: 0.5907\n",
      "Episode: 17 Total reward: 20.0 Training loss: 1.0816 Explore P: 0.5791\n",
      "Episode: 18 Total reward: 13.0 Training loss: 1.1927 Explore P: 0.5717\n",
      "Episode: 19 Total reward: 14.0 Training loss: 1.0749 Explore P: 0.5638\n",
      "Episode: 20 Total reward: 14.0 Training loss: 1.1196 Explore P: 0.5560\n",
      "Episode: 21 Total reward: 16.0 Training loss: 1.1448 Explore P: 0.5473\n",
      "Episode: 22 Total reward: 25.0 Training loss: 1.2113 Explore P: 0.5339\n",
      "Episode: 23 Total reward: 20.0 Training loss: 1.2673 Explore P: 0.5234\n",
      "Episode: 24 Total reward: 14.0 Training loss: 1.2648 Explore P: 0.5162\n",
      "Episode: 25 Total reward: 21.0 Training loss: 1.3252 Explore P: 0.5056\n",
      "Episode: 26 Total reward: 19.0 Training loss: 1.3708 Explore P: 0.4961\n",
      "Episode: 27 Total reward: 19.0 Training loss: 1.4528 Explore P: 0.4869\n",
      "Episode: 28 Total reward: 14.0 Training loss: 1.4944 Explore P: 0.4802\n",
      "Episode: 29 Total reward: 11.0 Training loss: 1.4831 Explore P: 0.4750\n",
      "Episode: 30 Total reward: 11.0 Training loss: 1.7137 Explore P: 0.4699\n",
      "Episode: 31 Total reward: 29.0 Training loss: 1.8621 Explore P: 0.4566\n",
      "Episode: 32 Total reward: 9.0 Training loss: 2.1258 Explore P: 0.4525\n",
      "Episode: 33 Total reward: 14.0 Training loss: 2.2378 Explore P: 0.4463\n",
      "Episode: 34 Total reward: 10.0 Training loss: 2.3499 Explore P: 0.4419\n",
      "Episode: 35 Total reward: 8.0 Training loss: 2.4146 Explore P: 0.4384\n",
      "Episode: 36 Total reward: 13.0 Training loss: 2.6382 Explore P: 0.4328\n",
      "Episode: 37 Total reward: 11.0 Training loss: 2.0085 Explore P: 0.4282\n",
      "Episode: 38 Total reward: 17.0 Training loss: 3.0964 Explore P: 0.4210\n",
      "Episode: 39 Total reward: 9.0 Training loss: 3.2540 Explore P: 0.4173\n",
      "Episode: 40 Total reward: 10.0 Training loss: 2.2963 Explore P: 0.4132\n",
      "Episode: 41 Total reward: 10.0 Training loss: 3.3422 Explore P: 0.4091\n",
      "Episode: 42 Total reward: 10.0 Training loss: 2.2526 Explore P: 0.4051\n",
      "Episode: 43 Total reward: 18.0 Training loss: 3.4411 Explore P: 0.3980\n",
      "Episode: 44 Total reward: 16.0 Training loss: 3.8822 Explore P: 0.3917\n",
      "Episode: 45 Total reward: 13.0 Training loss: 5.9341 Explore P: 0.3867\n",
      "Episode: 46 Total reward: 8.0 Training loss: 5.0135 Explore P: 0.3837\n",
      "Episode: 47 Total reward: 12.0 Training loss: 5.3461 Explore P: 0.3792\n",
      "Episode: 48 Total reward: 9.0 Training loss: 5.9046 Explore P: 0.3758\n",
      "Episode: 49 Total reward: 28.0 Training loss: 7.1170 Explore P: 0.3656\n",
      "Episode: 50 Total reward: 9.0 Training loss: 6.7287 Explore P: 0.3624\n",
      "Episode: 51 Total reward: 20.0 Training loss: 5.7701 Explore P: 0.3553\n",
      "Episode: 52 Total reward: 11.0 Training loss: 6.1831 Explore P: 0.3515\n",
      "Episode: 53 Total reward: 11.0 Training loss: 8.0319 Explore P: 0.3477\n",
      "Episode: 54 Total reward: 8.0 Training loss: 6.8588 Explore P: 0.3449\n",
      "Episode: 55 Total reward: 10.0 Training loss: 6.4344 Explore P: 0.3415\n",
      "Episode: 56 Total reward: 9.0 Training loss: 6.2030 Explore P: 0.3385\n",
      "Episode: 57 Total reward: 14.0 Training loss: 7.9380 Explore P: 0.3339\n",
      "Episode: 58 Total reward: 14.0 Training loss: 8.4079 Explore P: 0.3293\n",
      "Episode: 59 Total reward: 23.0 Training loss: 7.4696 Explore P: 0.3219\n",
      "Episode: 60 Total reward: 13.0 Training loss: 6.7305 Explore P: 0.3179\n",
      "Episode: 61 Total reward: 12.0 Training loss: 7.1927 Explore P: 0.3141\n",
      "Episode: 62 Total reward: 12.0 Training loss: 7.1597 Explore P: 0.3104\n",
      "Episode: 63 Total reward: 20.0 Training loss: 9.5478 Explore P: 0.3044\n",
      "Episode: 64 Total reward: 10.0 Training loss: 7.5003 Explore P: 0.3014\n",
      "Episode: 65 Total reward: 9.0 Training loss: 8.1321 Explore P: 0.2988\n",
      "Episode: 66 Total reward: 21.0 Training loss: 9.0591 Explore P: 0.2926\n",
      "Episode: 67 Total reward: 12.0 Training loss: 10.4113 Explore P: 0.2892\n",
      "Episode: 68 Total reward: 10.0 Training loss: 8.8303 Explore P: 0.2864\n",
      "Episode: 69 Total reward: 9.0 Training loss: 10.1774 Explore P: 0.2839\n",
      "Episode: 70 Total reward: 13.0 Training loss: 9.2529 Explore P: 0.2803\n",
      "Episode: 71 Total reward: 16.0 Training loss: 14.3838 Explore P: 0.2759\n",
      "Episode: 72 Total reward: 10.0 Training loss: 7.9110 Explore P: 0.2732\n",
      "Episode: 73 Total reward: 11.0 Training loss: 10.6716 Explore P: 0.2703\n",
      "Episode: 74 Total reward: 10.0 Training loss: 11.4250 Explore P: 0.2676\n",
      "Episode: 75 Total reward: 17.0 Training loss: 10.7862 Explore P: 0.2632\n",
      "Episode: 76 Total reward: 13.0 Training loss: 8.8288 Explore P: 0.2599\n",
      "Episode: 77 Total reward: 11.0 Training loss: 6.1769 Explore P: 0.2571\n",
      "Episode: 78 Total reward: 9.0 Training loss: 5.0971 Explore P: 0.2548\n",
      "Episode: 79 Total reward: 11.0 Training loss: 10.1082 Explore P: 0.2521\n",
      "Episode: 80 Total reward: 14.0 Training loss: 10.1538 Explore P: 0.2487\n",
      "Episode: 81 Total reward: 10.0 Training loss: 5.4666 Explore P: 0.2462\n",
      "Episode: 82 Total reward: 11.0 Training loss: 13.3645 Explore P: 0.2436\n",
      "Episode: 83 Total reward: 10.0 Training loss: 7.6774 Explore P: 0.2412\n",
      "Episode: 84 Total reward: 9.0 Training loss: 14.0745 Explore P: 0.2391\n",
      "Episode: 85 Total reward: 8.0 Training loss: 10.4240 Explore P: 0.2372\n",
      "Episode: 86 Total reward: 11.0 Training loss: 10.8953 Explore P: 0.2347\n",
      "Episode: 87 Total reward: 11.0 Training loss: 7.4426 Explore P: 0.2322\n",
      "Episode: 88 Total reward: 10.0 Training loss: 8.6645 Explore P: 0.2299\n",
      "Episode: 89 Total reward: 10.0 Training loss: 17.7133 Explore P: 0.2277\n",
      "Episode: 90 Total reward: 10.0 Training loss: 7.9821 Explore P: 0.2255\n",
      "Episode: 91 Total reward: 11.0 Training loss: 6.4069 Explore P: 0.2231\n",
      "Episode: 92 Total reward: 9.0 Training loss: 7.2762 Explore P: 0.2211\n",
      "Episode: 93 Total reward: 10.0 Training loss: 8.6688 Explore P: 0.2190\n",
      "Episode: 94 Total reward: 11.0 Training loss: 12.9767 Explore P: 0.2166\n",
      "Episode: 95 Total reward: 9.0 Training loss: 8.4568 Explore P: 0.2147\n",
      "Episode: 96 Total reward: 9.0 Training loss: 10.3086 Explore P: 0.2128\n",
      "Episode: 97 Total reward: 12.0 Training loss: 4.1060 Explore P: 0.2104\n",
      "Episode: 98 Total reward: 13.0 Training loss: 6.9704 Explore P: 0.2077\n",
      "Episode: 99 Total reward: 8.0 Training loss: 15.5390 Explore P: 0.2061\n",
      "Episode: 100 Total reward: 10.0 Training loss: 7.3185 Explore P: 0.2041\n",
      "Episode: 101 Total reward: 9.0 Training loss: 9.6290 Explore P: 0.2023\n",
      "Episode: 102 Total reward: 10.0 Training loss: 21.9428 Explore P: 0.2003\n",
      "Episode: 103 Total reward: 10.0 Training loss: 8.4914 Explore P: 0.1984\n",
      "Episode: 104 Total reward: 21.0 Training loss: 14.2695 Explore P: 0.1944\n",
      "Episode: 105 Total reward: 10.0 Training loss: 5.6667 Explore P: 0.1925\n",
      "Episode: 106 Total reward: 10.0 Training loss: 7.6586 Explore P: 0.1906\n",
      "Episode: 107 Total reward: 9.0 Training loss: 5.5005 Explore P: 0.1890\n",
      "Episode: 108 Total reward: 10.0 Training loss: 3.4386 Explore P: 0.1871\n",
      "Episode: 109 Total reward: 12.0 Training loss: 14.0613 Explore P: 0.1850\n",
      "Episode: 110 Total reward: 10.0 Training loss: 13.7209 Explore P: 0.1832\n",
      "Episode: 111 Total reward: 10.0 Training loss: 14.1039 Explore P: 0.1814\n",
      "Episode: 112 Total reward: 10.0 Training loss: 8.9061 Explore P: 0.1796\n",
      "Episode: 113 Total reward: 14.0 Training loss: 8.9724 Explore P: 0.1772\n",
      "Episode: 114 Total reward: 9.0 Training loss: 20.8476 Explore P: 0.1757\n",
      "Episode: 115 Total reward: 10.0 Training loss: 8.8182 Explore P: 0.1740\n",
      "Episode: 116 Total reward: 9.0 Training loss: 6.0838 Explore P: 0.1725\n",
      "Episode: 117 Total reward: 9.0 Training loss: 17.7261 Explore P: 0.1710\n",
      "Episode: 118 Total reward: 11.0 Training loss: 14.4497 Explore P: 0.1691\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 119 Total reward: 10.0 Training loss: 8.8294 Explore P: 0.1675\n",
      "Episode: 120 Total reward: 10.0 Training loss: 20.3174 Explore P: 0.1659\n",
      "Episode: 121 Total reward: 9.0 Training loss: 10.4323 Explore P: 0.1645\n",
      "Episode: 122 Total reward: 9.0 Training loss: 10.2809 Explore P: 0.1630\n",
      "Episode: 123 Total reward: 10.0 Training loss: 14.5799 Explore P: 0.1615\n",
      "Episode: 124 Total reward: 10.0 Training loss: 23.2913 Explore P: 0.1599\n",
      "Episode: 125 Total reward: 10.0 Training loss: 21.1204 Explore P: 0.1584\n",
      "Episode: 126 Total reward: 10.0 Training loss: 7.0101 Explore P: 0.1568\n",
      "Episode: 127 Total reward: 13.0 Training loss: 14.2922 Explore P: 0.1549\n",
      "Episode: 128 Total reward: 11.0 Training loss: 7.8685 Explore P: 0.1532\n",
      "Episode: 129 Total reward: 11.0 Training loss: 13.5430 Explore P: 0.1516\n",
      "Episode: 130 Total reward: 10.0 Training loss: 20.1675 Explore P: 0.1501\n",
      "Episode: 131 Total reward: 10.0 Training loss: 9.6325 Explore P: 0.1487\n",
      "Episode: 132 Total reward: 33.0 Training loss: 13.3119 Explore P: 0.1440\n",
      "Episode: 133 Total reward: 60.0 Training loss: 15.3224 Explore P: 0.1359\n",
      "Episode: 134 Total reward: 34.0 Training loss: 9.7396 Explore P: 0.1316\n",
      "Episode: 135 Total reward: 53.0 Training loss: 3.7408 Explore P: 0.1250\n",
      "Episode: 136 Total reward: 57.0 Training loss: 4.5566 Explore P: 0.1184\n",
      "Episode: 137 Total reward: 66.0 Training loss: 8.5868 Explore P: 0.1111\n",
      "Episode: 138 Total reward: 60.0 Training loss: 13.4972 Explore P: 0.1050\n",
      "Episode: 139 Total reward: 45.0 Training loss: 8.6428 Explore P: 0.1006\n",
      "Episode: 140 Total reward: 55.0 Training loss: 8.3392 Explore P: 0.0954\n",
      "Episode: 141 Total reward: 95.0 Training loss: 12.9209 Explore P: 0.0872\n",
      "Episode: 142 Total reward: 52.0 Training loss: 11.9341 Explore P: 0.0831\n",
      "Episode: 143 Total reward: 42.0 Training loss: 9.5271 Explore P: 0.0799\n",
      "Episode: 144 Total reward: 63.0 Training loss: 10.2783 Explore P: 0.0753\n",
      "Episode: 145 Total reward: 120.0 Training loss: 8.0550 Explore P: 0.0673\n",
      "Episode: 146 Total reward: 31.0 Training loss: 14.7823 Explore P: 0.0654\n",
      "Episode: 147 Total reward: 27.0 Training loss: 4.1179 Explore P: 0.0638\n",
      "Episode: 148 Total reward: 47.0 Training loss: 15.2796 Explore P: 0.0611\n",
      "Episode: 149 Total reward: 69.0 Training loss: 5.8084 Explore P: 0.0574\n",
      "Episode: 150 Total reward: 30.0 Training loss: 10.3189 Explore P: 0.0558\n",
      "Episode: 151 Total reward: 46.0 Training loss: 13.4641 Explore P: 0.0536\n",
      "Episode: 152 Total reward: 44.0 Training loss: 6.3624 Explore P: 0.0515\n",
      "Episode: 153 Total reward: 32.0 Training loss: 11.8327 Explore P: 0.0500\n",
      "Episode: 154 Total reward: 41.0 Training loss: 24.8775 Explore P: 0.0482\n",
      "Episode: 155 Total reward: 29.0 Training loss: 17.4497 Explore P: 0.0470\n",
      "Episode: 156 Total reward: 52.0 Training loss: 36.7487 Explore P: 0.0448\n",
      "Episode: 157 Total reward: 32.0 Training loss: 9.7889 Explore P: 0.0436\n",
      "Episode: 158 Total reward: 66.0 Training loss: 14.4836 Explore P: 0.0411\n",
      "Episode: 159 Total reward: 37.0 Training loss: 25.9844 Explore P: 0.0398\n",
      "Episode: 160 Total reward: 39.0 Training loss: 14.9256 Explore P: 0.0385\n",
      "Episode: 161 Total reward: 32.0 Training loss: 22.5666 Explore P: 0.0374\n",
      "Episode: 162 Total reward: 80.0 Training loss: 19.0878 Explore P: 0.0349\n",
      "Episode: 163 Total reward: 31.0 Training loss: 25.4570 Explore P: 0.0340\n",
      "Episode: 164 Total reward: 82.0 Training loss: 11.9955 Explore P: 0.0317\n",
      "Episode: 165 Total reward: 71.0 Training loss: 23.7094 Explore P: 0.0299\n",
      "Episode: 166 Total reward: 51.0 Training loss: 12.4741 Explore P: 0.0287\n",
      "Episode: 167 Total reward: 46.0 Training loss: 25.6311 Explore P: 0.0276\n",
      "Episode: 168 Total reward: 28.0 Training loss: 19.5904 Explore P: 0.0270\n",
      "Episode: 169 Total reward: 96.0 Training loss: 21.9822 Explore P: 0.0250\n",
      "Episode: 170 Total reward: 55.0 Training loss: 45.5912 Explore P: 0.0239\n",
      "Episode: 171 Total reward: 42.0 Training loss: 22.3676 Explore P: 0.0231\n",
      "Episode: 172 Total reward: 49.0 Training loss: 6.0802 Explore P: 0.0222\n",
      "Episode: 173 Total reward: 52.0 Training loss: 30.6862 Explore P: 0.0214\n",
      "Episode: 174 Total reward: 70.0 Training loss: 51.6743 Explore P: 0.0203\n",
      "Episode: 175 Total reward: 38.0 Training loss: 3.9050 Explore P: 0.0197\n",
      "Episode: 176 Total reward: 37.0 Training loss: 32.1816 Explore P: 0.0192\n",
      "Episode: 177 Total reward: 69.0 Training loss: 15.3650 Explore P: 0.0182\n",
      "Episode: 178 Total reward: 36.0 Training loss: 16.6311 Explore P: 0.0178\n",
      "Episode: 179 Total reward: 42.0 Training loss: 24.0224 Explore P: 0.0172\n",
      "Episode: 180 Total reward: 46.0 Training loss: 6.5825 Explore P: 0.0167\n",
      "Episode: 181 Total reward: 112.0 Training loss: 32.5577 Explore P: 0.0154\n",
      "Episode: 182 Total reward: 54.0 Training loss: 21.7804 Explore P: 0.0149\n",
      "Episode: 183 Total reward: 46.0 Training loss: 19.1931 Explore P: 0.0144\n",
      "Episode: 184 Total reward: 39.0 Training loss: 43.0681 Explore P: 0.0141\n",
      "Episode: 185 Total reward: 76.0 Training loss: 10.3205 Explore P: 0.0134\n",
      "Episode: 186 Total reward: 61.0 Training loss: 4.5442 Explore P: 0.0129\n",
      "Episode: 187 Total reward: 64.0 Training loss: 32.5437 Explore P: 0.0124\n",
      "Episode: 188 Total reward: 73.0 Training loss: 3.4675 Explore P: 0.0119\n",
      "Episode: 189 Total reward: 112.0 Training loss: 17.4181 Explore P: 0.0112\n",
      "Episode: 190 Total reward: 38.0 Training loss: 16.6181 Explore P: 0.0109\n",
      "Episode: 191 Total reward: 58.0 Training loss: 23.3881 Explore P: 0.0106\n",
      "Episode: 192 Total reward: 48.0 Training loss: 5.7454 Explore P: 0.0103\n",
      "Episode: 193 Total reward: 80.0 Training loss: 10.1246 Explore P: 0.0099\n",
      "Episode: 194 Total reward: 44.0 Training loss: 41.1644 Explore P: 0.0097\n",
      "Episode: 195 Total reward: 44.0 Training loss: 15.0211 Explore P: 0.0095\n",
      "Episode: 196 Total reward: 50.0 Training loss: 2.4345 Explore P: 0.0093\n",
      "Episode: 197 Total reward: 40.0 Training loss: 23.3567 Explore P: 0.0091\n",
      "Episode: 198 Total reward: 55.0 Training loss: 12.2949 Explore P: 0.0089\n",
      "Episode: 199 Total reward: 73.0 Training loss: 27.3145 Explore P: 0.0086\n",
      "Episode: 200 Total reward: 74.0 Training loss: 22.0810 Explore P: 0.0084\n",
      "Episode: 201 Total reward: 68.0 Training loss: 12.5272 Explore P: 0.0082\n",
      "Episode: 202 Total reward: 62.0 Training loss: 26.4774 Explore P: 0.0080\n",
      "Episode: 203 Total reward: 102.0 Training loss: 43.7514 Explore P: 0.0077\n",
      "Episode: 204 Total reward: 64.0 Training loss: 10.0550 Explore P: 0.0075\n",
      "Episode: 205 Total reward: 66.0 Training loss: 13.8973 Explore P: 0.0074\n",
      "Episode: 206 Total reward: 63.0 Training loss: 6.5769 Explore P: 0.0072\n",
      "Episode: 207 Total reward: 86.0 Training loss: 14.2732 Explore P: 0.0070\n",
      "Episode: 208 Total reward: 66.0 Training loss: 10.2452 Explore P: 0.0069\n",
      "Episode: 209 Total reward: 111.0 Training loss: 5.3945 Explore P: 0.0067\n",
      "Episode: 210 Total reward: 74.0 Training loss: 8.6135 Explore P: 0.0066\n",
      "Episode: 211 Total reward: 62.0 Training loss: 54.1156 Explore P: 0.0065\n",
      "Episode: 212 Total reward: 64.0 Training loss: 13.5226 Explore P: 0.0064\n",
      "Episode: 213 Total reward: 69.0 Training loss: 4.0231 Explore P: 0.0063\n",
      "Episode: 214 Total reward: 59.0 Training loss: 2.8226 Explore P: 0.0062\n",
      "Episode: 215 Total reward: 99.0 Training loss: 61.6387 Explore P: 0.0061\n",
      "Episode: 216 Total reward: 110.0 Training loss: 11.0733 Explore P: 0.0060\n",
      "Episode: 217 Total reward: 121.0 Training loss: 12.3939 Explore P: 0.0059\n",
      "Episode: 218 Total reward: 110.0 Training loss: 28.3909 Explore P: 0.0058\n",
      "Episode: 219 Total reward: 97.0 Training loss: 4.4460 Explore P: 0.0057\n",
      "Episode: 220 Total reward: 73.0 Training loss: 33.0967 Explore P: 0.0057\n",
      "Episode: 221 Total reward: 96.0 Training loss: 6.4473 Explore P: 0.0056\n",
      "Episode: 222 Total reward: 119.0 Training loss: 13.8182 Explore P: 0.0055\n",
      "Episode: 223 Total reward: 116.0 Training loss: 5.1558 Explore P: 0.0055\n",
      "Episode: 224 Total reward: 127.0 Training loss: 12.8511 Explore P: 0.0054\n",
      "Episode: 225 Total reward: 141.0 Training loss: 16.3056 Explore P: 0.0054\n",
      "Episode: 226 Total reward: 183.0 Training loss: 20.8185 Explore P: 0.0053\n",
      "Episode: 227 Total reward: 158.0 Training loss: 5.7951 Explore P: 0.0053\n",
      "Episode: 228 Total reward: 119.0 Training loss: 12.7062 Explore P: 0.0052\n",
      "Episode: 229 Total reward: 146.0 Training loss: 4.1579 Explore P: 0.0052\n",
      "Episode: 230 Total reward: 169.0 Training loss: 22.2269 Explore P: 0.0052\n",
      "Episode: 231 Total reward: 178.0 Training loss: 27.8214 Explore P: 0.0051\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 232 Total reward: 184.0 Training loss: 3.9256 Explore P: 0.0051\n",
      "Episode: 233 Total reward: 186.0 Training loss: 3.3394 Explore P: 0.0051\n",
      "Episode: 234 Total reward: 168.0 Training loss: 11.1755 Explore P: 0.0051\n",
      "Episode: 235 Total reward: 200.0 Training loss: 18.4670 Explore P: 0.0051\n",
      "Episode: 236 Total reward: 173.0 Training loss: 7.0471 Explore P: 0.0051\n",
      "Episode: 237 Total reward: 174.0 Training loss: 54.2775 Explore P: 0.0050\n",
      "Episode: 238 Total reward: 190.0 Training loss: 3.6739 Explore P: 0.0050\n",
      "Episode: 239 Total reward: 200.0 Training loss: 21.5240 Explore P: 0.0050\n",
      "Episode: 240 Total reward: 200.0 Training loss: 15.0186 Explore P: 0.0050\n",
      "Episode: 241 Total reward: 200.0 Training loss: 5.7265 Explore P: 0.0050\n",
      "Episode: 242 Total reward: 200.0 Training loss: 14.8918 Explore P: 0.0050\n",
      "Episode: 243 Total reward: 200.0 Training loss: 5.7049 Explore P: 0.0050\n",
      "Episode: 244 Total reward: 200.0 Training loss: 30.0618 Explore P: 0.0050\n",
      "Episode: 245 Total reward: 200.0 Training loss: 6.8492 Explore P: 0.0050\n",
      "Episode: 246 Total reward: 200.0 Training loss: 49.7903 Explore P: 0.0050\n",
      "Episode: 247 Total reward: 200.0 Training loss: 2.0087 Explore P: 0.0050\n",
      "Episode: 248 Total reward: 200.0 Training loss: 1.9510 Explore P: 0.0050\n",
      "Episode: 249 Total reward: 159.0 Training loss: 28.8050 Explore P: 0.0050\n",
      "Episode: 250 Total reward: 200.0 Training loss: 1.4851 Explore P: 0.0050\n",
      "Episode: 251 Total reward: 102.0 Training loss: 1.7123 Explore P: 0.0050\n",
      "Episode: 252 Total reward: 133.0 Training loss: 21.2928 Explore P: 0.0050\n",
      "Episode: 253 Total reward: 74.0 Training loss: 18.4800 Explore P: 0.0050\n",
      "Episode: 254 Total reward: 114.0 Training loss: 143.2921 Explore P: 0.0050\n",
      "Episode: 255 Total reward: 161.0 Training loss: 2.4595 Explore P: 0.0050\n",
      "Episode: 256 Total reward: 138.0 Training loss: 17.9854 Explore P: 0.0050\n",
      "Episode: 257 Total reward: 155.0 Training loss: 2.5092 Explore P: 0.0050\n",
      "Episode: 258 Total reward: 200.0 Training loss: 1.5487 Explore P: 0.0050\n",
      "Episode: 259 Total reward: 185.0 Training loss: 1.8217 Explore P: 0.0050\n",
      "Episode: 260 Total reward: 200.0 Training loss: 51.9356 Explore P: 0.0050\n",
      "Episode: 261 Total reward: 200.0 Training loss: 67.0210 Explore P: 0.0050\n",
      "Episode: 262 Total reward: 200.0 Training loss: 37.0213 Explore P: 0.0050\n",
      "Episode: 263 Total reward: 200.0 Training loss: 2.2254 Explore P: 0.0050\n",
      "Episode: 264 Total reward: 200.0 Training loss: 37.9635 Explore P: 0.0050\n",
      "Episode: 265 Total reward: 172.0 Training loss: 81.3967 Explore P: 0.0050\n",
      "Episode: 266 Total reward: 200.0 Training loss: 37.9485 Explore P: 0.0050\n",
      "Episode: 267 Total reward: 200.0 Training loss: 43.1518 Explore P: 0.0050\n",
      "Episode: 268 Total reward: 200.0 Training loss: 1.4050 Explore P: 0.0050\n",
      "Episode: 269 Total reward: 200.0 Training loss: 0.8624 Explore P: 0.0050\n",
      "Episode: 270 Total reward: 200.0 Training loss: 102.7204 Explore P: 0.0050\n",
      "Episode: 271 Total reward: 200.0 Training loss: 3.1203 Explore P: 0.0050\n",
      "Episode: 272 Total reward: 200.0 Training loss: 1.6917 Explore P: 0.0050\n",
      "Episode: 273 Total reward: 200.0 Training loss: 36.7006 Explore P: 0.0050\n",
      "Episode: 274 Total reward: 200.0 Training loss: 39.1388 Explore P: 0.0050\n",
      "Episode: 275 Total reward: 200.0 Training loss: 1.1521 Explore P: 0.0050\n",
      "Episode: 276 Total reward: 200.0 Training loss: 133.4915 Explore P: 0.0050\n",
      "Episode: 277 Total reward: 200.0 Training loss: 174.3300 Explore P: 0.0050\n",
      "Episode: 278 Total reward: 200.0 Training loss: 1.0976 Explore P: 0.0050\n",
      "Episode: 279 Total reward: 200.0 Training loss: 75.8516 Explore P: 0.0050\n",
      "Episode: 280 Total reward: 200.0 Training loss: 0.7446 Explore P: 0.0050\n",
      "Episode: 281 Total reward: 200.0 Training loss: 60.4045 Explore P: 0.0050\n",
      "Episode: 282 Total reward: 200.0 Training loss: 0.7380 Explore P: 0.0050\n",
      "Episode: 283 Total reward: 200.0 Training loss: 1.0866 Explore P: 0.0050\n",
      "Episode: 284 Total reward: 200.0 Training loss: 0.6174 Explore P: 0.0050\n",
      "Episode: 285 Total reward: 200.0 Training loss: 0.5845 Explore P: 0.0050\n",
      "Episode: 286 Total reward: 200.0 Training loss: 0.5508 Explore P: 0.0050\n",
      "Episode: 287 Total reward: 200.0 Training loss: 0.6172 Explore P: 0.0050\n",
      "Episode: 288 Total reward: 200.0 Training loss: 0.6406 Explore P: 0.0050\n",
      "Episode: 289 Total reward: 200.0 Training loss: 1.7277 Explore P: 0.0050\n",
      "Episode: 290 Total reward: 200.0 Training loss: 0.5570 Explore P: 0.0050\n",
      "Episode: 291 Total reward: 200.0 Training loss: 0.4085 Explore P: 0.0050\n",
      "Episode: 292 Total reward: 200.0 Training loss: 0.2881 Explore P: 0.0050\n",
      "Episode: 293 Total reward: 200.0 Training loss: 5.8792 Explore P: 0.0050\n",
      "Episode: 294 Total reward: 200.0 Training loss: 60.7917 Explore P: 0.0050\n",
      "Episode: 295 Total reward: 200.0 Training loss: 60.5515 Explore P: 0.0050\n",
      "Episode: 296 Total reward: 200.0 Training loss: 53.9616 Explore P: 0.0050\n",
      "Episode: 297 Total reward: 200.0 Training loss: 0.3538 Explore P: 0.0050\n",
      "Episode: 298 Total reward: 200.0 Training loss: 0.3899 Explore P: 0.0050\n",
      "Episode: 299 Total reward: 200.0 Training loss: 0.4373 Explore P: 0.0050\n",
      "Episode: 300 Total reward: 200.0 Training loss: 57.5526 Explore P: 0.0050\n",
      "Episode: 301 Total reward: 159.0 Training loss: 60.4006 Explore P: 0.0050\n",
      "Episode: 302 Total reward: 200.0 Training loss: 0.5460 Explore P: 0.0050\n",
      "Episode: 303 Total reward: 200.0 Training loss: 0.3064 Explore P: 0.0050\n",
      "Episode: 304 Total reward: 200.0 Training loss: 117.0602 Explore P: 0.0050\n",
      "Episode: 305 Total reward: 200.0 Training loss: 1.3946 Explore P: 0.0050\n",
      "Episode: 306 Total reward: 137.0 Training loss: 20.2029 Explore P: 0.0050\n",
      "Episode: 307 Total reward: 200.0 Training loss: 58.3369 Explore P: 0.0050\n",
      "Episode: 308 Total reward: 141.0 Training loss: 29.5485 Explore P: 0.0050\n",
      "Episode: 309 Total reward: 200.0 Training loss: 64.1405 Explore P: 0.0050\n",
      "Episode: 310 Total reward: 194.0 Training loss: 0.3417 Explore P: 0.0050\n",
      "Episode: 311 Total reward: 200.0 Training loss: 0.4496 Explore P: 0.0050\n",
      "Episode: 312 Total reward: 200.0 Training loss: 0.3631 Explore P: 0.0050\n",
      "Episode: 313 Total reward: 200.0 Training loss: 0.2945 Explore P: 0.0050\n",
      "Episode: 314 Total reward: 200.0 Training loss: 0.2776 Explore P: 0.0050\n",
      "Episode: 315 Total reward: 200.0 Training loss: 56.1044 Explore P: 0.0050\n",
      "Episode: 316 Total reward: 200.0 Training loss: 0.3918 Explore P: 0.0050\n",
      "Episode: 317 Total reward: 200.0 Training loss: 0.2283 Explore P: 0.0050\n",
      "Episode: 318 Total reward: 149.0 Training loss: 0.2408 Explore P: 0.0050\n",
      "Episode: 319 Total reward: 200.0 Training loss: 42.6134 Explore P: 0.0050\n",
      "Episode: 320 Total reward: 200.0 Training loss: 2.2306 Explore P: 0.0050\n",
      "Episode: 321 Total reward: 200.0 Training loss: 5.3391 Explore P: 0.0050\n",
      "Episode: 322 Total reward: 200.0 Training loss: 23.5880 Explore P: 0.0050\n",
      "Episode: 323 Total reward: 200.0 Training loss: 0.2555 Explore P: 0.0050\n",
      "Episode: 324 Total reward: 200.0 Training loss: 0.2889 Explore P: 0.0050\n",
      "Episode: 325 Total reward: 200.0 Training loss: 42.0408 Explore P: 0.0050\n",
      "Episode: 326 Total reward: 200.0 Training loss: 0.3165 Explore P: 0.0050\n",
      "Episode: 327 Total reward: 200.0 Training loss: 157.4980 Explore P: 0.0050\n",
      "Episode: 328 Total reward: 182.0 Training loss: 0.2764 Explore P: 0.0050\n",
      "Episode: 329 Total reward: 147.0 Training loss: 0.3055 Explore P: 0.0050\n",
      "Episode: 330 Total reward: 176.0 Training loss: 4.2195 Explore P: 0.0050\n",
      "Episode: 331 Total reward: 134.0 Training loss: 8.5658 Explore P: 0.0050\n",
      "Episode: 332 Total reward: 147.0 Training loss: 0.3138 Explore P: 0.0050\n",
      "Episode: 333 Total reward: 200.0 Training loss: 54.9368 Explore P: 0.0050\n",
      "Episode: 334 Total reward: 200.0 Training loss: 0.2705 Explore P: 0.0050\n",
      "Episode: 335 Total reward: 200.0 Training loss: 63.1057 Explore P: 0.0050\n",
      "Episode: 336 Total reward: 200.0 Training loss: 0.2025 Explore P: 0.0050\n",
      "Episode: 337 Total reward: 200.0 Training loss: 0.2315 Explore P: 0.0050\n",
      "Episode: 338 Total reward: 200.0 Training loss: 0.2143 Explore P: 0.0050\n",
      "Episode: 339 Total reward: 200.0 Training loss: 1.8864 Explore P: 0.0050\n",
      "Episode: 340 Total reward: 200.0 Training loss: 0.2692 Explore P: 0.0050\n",
      "Episode: 341 Total reward: 200.0 Training loss: 0.2732 Explore P: 0.0050\n",
      "Episode: 342 Total reward: 200.0 Training loss: 0.2511 Explore P: 0.0050\n",
      "Episode: 343 Total reward: 200.0 Training loss: 0.2637 Explore P: 0.0050\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 344 Total reward: 200.0 Training loss: 0.2668 Explore P: 0.0050\n",
      "Episode: 345 Total reward: 200.0 Training loss: 21.1654 Explore P: 0.0050\n",
      "Episode: 346 Total reward: 200.0 Training loss: 0.2571 Explore P: 0.0050\n",
      "Episode: 347 Total reward: 200.0 Training loss: 0.2399 Explore P: 0.0050\n",
      "Episode: 348 Total reward: 200.0 Training loss: 0.2978 Explore P: 0.0050\n",
      "Episode: 349 Total reward: 200.0 Training loss: 0.2801 Explore P: 0.0050\n",
      "Episode: 350 Total reward: 200.0 Training loss: 0.2273 Explore P: 0.0050\n",
      "Episode: 351 Total reward: 200.0 Training loss: 56.9008 Explore P: 0.0050\n",
      "Episode: 352 Total reward: 200.0 Training loss: 48.4167 Explore P: 0.0050\n",
      "Episode: 353 Total reward: 200.0 Training loss: 6.5636 Explore P: 0.0050\n",
      "Episode: 354 Total reward: 200.0 Training loss: 31.8980 Explore P: 0.0050\n",
      "Episode: 355 Total reward: 200.0 Training loss: 0.3215 Explore P: 0.0050\n",
      "Episode: 356 Total reward: 200.0 Training loss: 0.2865 Explore P: 0.0050\n",
      "Episode: 357 Total reward: 200.0 Training loss: 0.3801 Explore P: 0.0050\n",
      "Episode: 358 Total reward: 200.0 Training loss: 0.2640 Explore P: 0.0050\n",
      "Episode: 359 Total reward: 200.0 Training loss: 33.2169 Explore P: 0.0050\n",
      "Episode: 360 Total reward: 200.0 Training loss: 12.5193 Explore P: 0.0050\n",
      "Episode: 361 Total reward: 200.0 Training loss: 0.4032 Explore P: 0.0050\n",
      "Episode: 362 Total reward: 200.0 Training loss: 0.3782 Explore P: 0.0050\n",
      "Episode: 363 Total reward: 200.0 Training loss: 24.6358 Explore P: 0.0050\n",
      "Episode: 364 Total reward: 200.0 Training loss: 0.3416 Explore P: 0.0050\n",
      "Episode: 365 Total reward: 200.0 Training loss: 0.5232 Explore P: 0.0050\n",
      "Episode: 366 Total reward: 200.0 Training loss: 18.1537 Explore P: 0.0050\n",
      "Episode: 367 Total reward: 200.0 Training loss: 0.3285 Explore P: 0.0050\n",
      "Episode: 368 Total reward: 200.0 Training loss: 0.2930 Explore P: 0.0050\n",
      "Episode: 369 Total reward: 200.0 Training loss: 77.5774 Explore P: 0.0050\n",
      "Episode: 370 Total reward: 200.0 Training loss: 0.3800 Explore P: 0.0050\n",
      "Episode: 371 Total reward: 200.0 Training loss: 65.2127 Explore P: 0.0050\n",
      "Episode: 372 Total reward: 200.0 Training loss: 0.3207 Explore P: 0.0050\n",
      "Episode: 373 Total reward: 200.0 Training loss: 0.4842 Explore P: 0.0050\n",
      "Episode: 374 Total reward: 200.0 Training loss: 21.8623 Explore P: 0.0050\n",
      "Episode: 375 Total reward: 200.0 Training loss: 30.7960 Explore P: 0.0050\n",
      "Episode: 376 Total reward: 200.0 Training loss: 0.2754 Explore P: 0.0050\n",
      "Episode: 377 Total reward: 200.0 Training loss: 30.2291 Explore P: 0.0050\n",
      "Episode: 378 Total reward: 200.0 Training loss: 0.6372 Explore P: 0.0050\n",
      "Episode: 379 Total reward: 200.0 Training loss: 0.4851 Explore P: 0.0050\n",
      "Episode: 380 Total reward: 200.0 Training loss: 0.2747 Explore P: 0.0050\n",
      "Episode: 381 Total reward: 200.0 Training loss: 0.2423 Explore P: 0.0050\n",
      "Episode: 382 Total reward: 200.0 Training loss: 0.2422 Explore P: 0.0050\n",
      "Episode: 383 Total reward: 200.0 Training loss: 24.8595 Explore P: 0.0050\n",
      "Episode: 384 Total reward: 200.0 Training loss: 0.3475 Explore P: 0.0050\n",
      "Episode: 385 Total reward: 200.0 Training loss: 0.2737 Explore P: 0.0050\n",
      "Episode: 386 Total reward: 200.0 Training loss: 70.0484 Explore P: 0.0050\n",
      "Episode: 387 Total reward: 200.0 Training loss: 0.3616 Explore P: 0.0050\n",
      "Episode: 388 Total reward: 200.0 Training loss: 40.9076 Explore P: 0.0050\n",
      "Episode: 389 Total reward: 200.0 Training loss: 0.3309 Explore P: 0.0050\n",
      "Episode: 390 Total reward: 200.0 Training loss: 0.3489 Explore P: 0.0050\n",
      "Episode: 391 Total reward: 95.0 Training loss: 31.2253 Explore P: 0.0050\n",
      "Episode: 392 Total reward: 93.0 Training loss: 0.6111 Explore P: 0.0050\n",
      "Episode: 393 Total reward: 95.0 Training loss: 0.5938 Explore P: 0.0050\n",
      "Episode: 394 Total reward: 87.0 Training loss: 13.9994 Explore P: 0.0050\n",
      "Episode: 395 Total reward: 95.0 Training loss: 26.1027 Explore P: 0.0050\n",
      "Episode: 396 Total reward: 102.0 Training loss: 35.7386 Explore P: 0.0050\n",
      "Episode: 397 Total reward: 91.0 Training loss: 101.0820 Explore P: 0.0050\n",
      "Episode: 398 Total reward: 93.0 Training loss: 1.3834 Explore P: 0.0050\n",
      "Episode: 399 Total reward: 43.0 Training loss: 1.0466 Explore P: 0.0050\n"
     ]
    }
   ],
   "source": [
    "rewards_list = []\n",
    "\n",
    "step = 0\n",
    "for ep in range(train_episodes):\n",
    "    total_reward = 0\n",
    "    t = 0\n",
    "    state = env.reset()  # Reset and get initial state\n",
    "    while t < max_steps:\n",
    "        step += 1\n",
    "        # Uncomment this next line to watch the training\n",
    "        # env.render() \n",
    "            \n",
    "        # Explore or exploit\n",
    "        explore_p = explore_stop + (explore_start - explore_stop)*np.exp(-decay_rate*step) \n",
    "        if explore_p > np.random.rand():\n",
    "            # Make a random action\n",
    "            action = env.action_space.sample()\n",
    "        else:\n",
    "            # Get action from Q-network\n",
    "            Qs = mainQN(np.resize(state, (1, state_size)).astype(np.float32))\n",
    "            action = np.argmax(Qs)\n",
    "            \n",
    "        # Take action, get new state and reward\n",
    "        next_state, reward, done, _ = env.step(action)\n",
    "    \n",
    "        total_reward += reward\n",
    "           \n",
    "        if done:\n",
    "            # Episode ends, so no next state\n",
    "            next_state = np.zeros(state.shape)\n",
    "            t = max_steps\n",
    "                \n",
    "            print('Episode: {}'.format(ep), 'Total reward: {}'.format(total_reward),\n",
    "                  'Training loss: {:.4f}'.format(loss), 'Explore P: {:.4f}'.format(explore_p))\n",
    "            rewards_list.append((ep, total_reward))\n",
    "                \n",
    "            # Add experience to memory\n",
    "            memory.add((state, action, reward, next_state))\n",
    "        else:\n",
    "            # Add experience to memory\n",
    "            memory.add((state, action, reward, next_state))\n",
    "            state = next_state\n",
    "            t += 1\n",
    "            \n",
    "        # Sample mini-batch from memory\n",
    "        batch = memory.sample(batch_size)\n",
    "        states = np.array([each[0] for each in batch])\n",
    "        actions = np.array([each[1] for each in batch])\n",
    "        rewards = np.array([each[2] for each in batch])\n",
    "        next_states = np.array([each[3] for each in batch])\n",
    "            \n",
    "        # Train network           \n",
    "        target_Qs = mainQN(next_states.astype(np.float32)).numpy()\n",
    "            \n",
    "        # Set target_Qs to 0 for states where episode ends\n",
    "        episode_ends = (next_states == np.zeros(states[0].shape)).all(axis=1)\n",
    "        target_Qs[episode_ends] = (0., 0.)\n",
    "            \n",
    "        # Compute target Q values   \n",
    "        targets = rewards + gamma * np.max(target_Qs, axis=1)  # Max: Q-learning is off-policy & greedy \n",
    "\n",
    "        # Gradient-based update\n",
    "        loss, gradients = compute_gradient(states.astype(np.float32), actions, targets.astype(np.float32))\n",
    "        optimizer.apply_gradients(zip(gradients, mainQN.trainable_variables))\n",
    "\n",
    "log_path = \"/tmp/deep_Q_network\"\n",
    "mainQN.save_weights(log_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluate learning process and final policy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Moving average for smoothing plot\n",
    "def running_mean(x, N):\n",
    "    cumsum = np.cumsum(np.insert(x, 0, 0)) \n",
    "    return (cumsum[N:] - cumsum[:-N]) / N"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0, 0.5, 'Total Reward')"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "eps, rews = np.array(rewards_list).T\n",
    "smoothed_rews = running_mean(rews, 10)\n",
    "plt.plot(eps[-len(smoothed_rews):], smoothed_rews)\n",
    "plt.plot(eps, rews, color='grey', alpha=0.3)\n",
    "plt.xlabel('Episode')\n",
    "plt.ylabel('Total Reward')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "initial state: [-0.0325887   0.03579323 -0.0167991  -0.03179098]\n",
      "number of steps: 35.0\n",
      "initial state: [-0.00491421 -0.04861907  0.04852777  0.03435305]\n",
      "number of steps: 38.0\n",
      "initial state: [-0.0373284   0.03572017 -0.03216222  0.00256773]\n",
      "number of steps: 28.0\n",
      "initial state: [ 0.0482177  -0.02838264  0.04230335  0.03683343]\n",
      "number of steps: 35.0\n",
      "initial state: [-0.0062584   0.0169086   0.02276069  0.00459129]\n",
      "number of steps: 36.0\n",
      "initial state: [ 0.04068825 -0.04142618  0.00656804  0.03761432]\n",
      "number of steps: 36.0\n",
      "initial state: [ 0.03311615 -0.03559329 -0.03074419 -0.03235361]\n",
      "number of steps: 31.0\n",
      "initial state: [-0.01756248 -0.04830539  0.04905984  0.03037402]\n",
      "number of steps: 36.0\n",
      "initial state: [-0.0024902   0.04219503  0.00342888 -0.0027845 ]\n",
      "number of steps: 31.0\n",
      "initial state: [ 0.0129798   0.01527522 -0.04913573  0.01069316]\n",
      "number of steps: 25.0\n"
     ]
    }
   ],
   "source": [
    "testQN = QNetwork(name='test', hidden_size=hidden_size)\n",
    "testQN.build(input_shape=(None, state_size))\n",
    "testQN.load_weights(log_path)\n",
    "\n",
    "test_episodes = 10\n",
    "\n",
    "for ep in range(test_episodes):\n",
    "    state = env.reset()\n",
    "    print(\"initial state:\", state)\n",
    "    R = 0\n",
    "    while True:\n",
    "        env.render() \n",
    "            \n",
    "        # Get action from Q-network\n",
    "        # Hm, the following line could be more elegant ...\n",
    "        Qs = testQN(np.float32(np.resize(state, (1, state_size))))\n",
    "        action = np.argmax(Qs)\n",
    "            \n",
    "        # Take action, get new state and reward\n",
    "        next_state, reward, done, _ = env.step(action)\n",
    "        R += reward\n",
    "            \n",
    "        if done:\n",
    "            print(\"number of steps:\", R)\n",
    "            break\n",
    "        else:\n",
    "            state = next_state\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
