{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tabular Q-learning using a linear model in TF2\n",
    "## Christian Igel, 2019\n",
    "\n",
    "This example implements tabular Q-learning via a linear model and applies it to simple gridworlds. If you have suggestions for improvement, [let me know](mailto:igel@diku.dk)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "import gym\n",
    "import gym_gridworlds  # pip install gym-gridworlds\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose either of the two environments\n",
    "env_name = 'CliffWalking-v0'\n",
    "# env_name = 'FrozenLake-v0'\n",
    "env = gym.make(env_name)  \n",
    "\n",
    "env.render()\n",
    "\n",
    "number_of_actions = env.action_space.n\n",
    "number_of_states = env.observation_space.n\n",
    "print(\"|S| =\", number_of_states)\n",
    "print(\"|A| =\", number_of_actions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define linear model\n",
    "#x = tf.keras.Input(shape=(number_of_states,), dtype=tf.float64)  # input state\n",
    "x = tf.keras.Input(shape=(number_of_states,))  # input state\n",
    "y = tf.keras.layers.Dense(number_of_actions, activation=None, use_bias=False, \n",
    "                          kernel_initializer=tf.keras.initializers.RandomUniform(0, 0.01))(x)\n",
    "#argmax_y = tf.argmax(y, 1) # best action\n",
    "\n",
    "# Instantiate model\n",
    "model = tf.keras.Model(inputs=x, outputs=y)\n",
    "\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eta = 0.1  # (initial) learning rate\n",
    "optimizer = tf.keras.optimizers.SGD(learning_rate=eta)\n",
    "\n",
    "@tf.function\n",
    "def compute_gradient(x, Q_target):\n",
    "    with tf.GradientTape() as tape:\n",
    "            Q = model(x)\n",
    "            loss = tf.math.reduce_mean(tf.square(Q - Q_target))\n",
    "    return tape.gradient(loss, model.trainable_variables)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set learning parameters\n",
    "gamma = 1. #.99  # gamma\n",
    "initial_epsilon = epsilon = 0.1  # epsilon for epsilon-greedy selection\n",
    "number_of_episodes = 2000\n",
    "max_number_of_steps = 100\n",
    "T_list = []  # list gathering maximum number of steps for each episode\n",
    "R_list = [] \n",
    "\n",
    "for i in tqdm(range(number_of_episodes)):\n",
    "    s = env.reset()  # reset environment and get first state\n",
    "    R = 0  # return (accumulated reward)\n",
    "    for t in range(max_number_of_steps):  # maximum number of steps\n",
    "        # Choose an action greedily (with e chance of random action) from the Q-network\n",
    "        Q = model(np.eye(1, number_of_states, s, dtype=np.float32))\n",
    "        a = np.argmax(Q, 1) # best action\n",
    "        if np.random.rand(1) < epsilon:\n",
    "            a[0] = env.action_space.sample()\n",
    "        # Observe new state and reward from environment\n",
    "        s_prime, r, d, _ = env.step(a[0])\n",
    "        # Compute Q' by feeding the new state into the network\n",
    "        Q_prime = model(np.eye(1, number_of_states, s_prime, dtype=np.float32))\n",
    "        # Compute maximum value of Q_prime and set  target value for chosen action\n",
    "        max_Q_prime = np.max(Q_prime)\n",
    "        Q_target = Q.numpy()\n",
    "        Q_target[0, a[0]] = r + gamma * max_Q_prime\n",
    "        # Train network using target and predicted Q values\n",
    "        gradients = compute_gradient(np.eye(1, number_of_states, s, dtype=np.float32), Q_target); \n",
    "        optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
    "        R += r\n",
    "        s = s_prime\n",
    "        if d == True:  # episode ended\n",
    "            # Reduce probability of random actions over time\n",
    "            epsilon = 1./((i/50) + (1./initial_epsilon))\n",
    "            break\n",
    "    T_list.append(t)\n",
    "    R_list.append(R)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if env_name == 'FrozenLake-v0':\n",
    "    print(\"Percent of succesful episodes:\", sum(R_list)/number_of_episodes)\n",
    "plt.plot(R_list, 'g.')\n",
    "plt.show()\n",
    "plt.plot(T_list, 'b.')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.render()\n",
    "\n",
    "weights = model.get_weights()\n",
    "\n",
    "if env_name == 'FrozenLake-v0':\n",
    "    print(\"V:\\n\", np.around(np.max(weights, 2).reshape((4,4)), decimals=1))\n",
    "    print(\"actions:\\n\", np.argmax(weights, 2).reshape((4,4)))\n",
    "if env_name == 'CliffWalking-v0':\n",
    "    print(\"V:\\n\", np.around(np.max(weights, 2).reshape((4,12)), decimals=1))\n",
    "    print(\"actions:\\n\", np.argmax(weights, 2).reshape((4,12)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
